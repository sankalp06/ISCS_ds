{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier  # Replace with your desired classifier\n",
    "\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def separate_features_target(df, target_column):\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "    return X, y\n",
    "\n",
    "def identify_features(X):\n",
    "    numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns\n",
    "    return numerical_features, categorical_features\n",
    "\n",
    "def create_preprocessor():\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "    return preprocessor\n",
    "\n",
    "def create_classifier():\n",
    "    return RandomForestClassifier()  # Replace with your desired classifier\n",
    "\n",
    "def create_pipeline(preprocessor, classifier):\n",
    "    return Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "\n",
    "def train_pipeline(pipeline, X_train, y_train):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "def evaluate_pipeline(pipeline, X_test, y_test):\n",
    "    accuracy = pipeline.score(X_test, y_test)\n",
    "    print(f'Model Accuracy: {accuracy}')\n",
    "\n",
    "# Load CSV data into a DataFrame\n",
    "file_path = 'your_data.csv'\n",
    "df = load_data(file_path)\n",
    "\n",
    "# Separate features and target variable\n",
    "X, y = separate_features_target(df, 'target_variable')\n",
    "\n",
    "# Identify numerical and categorical features\n",
    "numerical_features, categorical_features = identify_features(X)\n",
    "\n",
    "# Create preprocessor and classifier\n",
    "preprocessor = create_preprocessor()\n",
    "classifier = create_classifier()\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = create_pipeline(preprocessor, classifier)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the pipeline on the training data\n",
    "train_pipeline(pipeline, X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "evaluate_pipeline(pipeline, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import joblib\n",
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "  \n",
    "\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def fetch_and_merge_dataset(dataset_id):\n",
    "    # Fetch dataset\n",
    "    dataset = fetch_ucirepo(id=dataset_id)\n",
    "    \n",
    "    # Extract features and targets\n",
    "    X = dataset.data.features\n",
    "    y = dataset.data.targets\n",
    "    \n",
    "    # Merge X and y into a single DataFrame\n",
    "    df = pd.concat([X, pd.DataFrame({'target': y})], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def separate_features_target(df, target_column):\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "    return X, y\n",
    "\n",
    "def identify_features(X):\n",
    "    numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns\n",
    "    return numerical_features, categorical_features\n",
    "\n",
    "def handle_duplicates(df):\n",
    "    df_no_duplicates = df.drop_duplicates()\n",
    "    return df_no_duplicates\n",
    "\n",
    "def create_preprocessor():\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "    return preprocessor\n",
    "\n",
    "def create_classifier():\n",
    "    return RandomForestClassifier()\n",
    "\n",
    "def create_pipeline(preprocessor, classifier):\n",
    "    return Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "\n",
    "def train_pipeline(pipeline, X_train, y_train):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "def evaluate_pipeline(pipeline, X_test, y_test):\n",
    "    accuracy = pipeline.score(X_test, y_test)\n",
    "    print(f'Model Accuracy: {accuracy}')\n",
    "\n",
    "def save_pipeline(pipeline, file_path):\n",
    "    joblib.dump(pipeline, file_path)\n",
    "\n",
    "\n",
    "\n",
    "def load_pipeline(file_path):\n",
    "    return joblib.load(file_path)\n",
    "\n",
    "def predict_with_pipeline(pipeline, new_data):\n",
    "    return pipeline.predict(new_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.8785046728971962\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "# fetch dataset \n",
    "aids = fetch_ucirepo(id=890) \n",
    "# data (as pandas dataframes) \n",
    "X_ = aids.data.features \n",
    "y_ = aids.data.targets \n",
    "# Merge X and y into a single DataFrame\n",
    "df = pd.concat([X_, y_], axis=1)\n",
    "df = handle_duplicates(df)\n",
    "# Separate features and target variable\n",
    "X, y = separate_features_target(df, 'cid')\n",
    "# Identify numerical and categorical features\n",
    "numerical_features, categorical_features = identify_features(X)\n",
    "# Create preprocessor and classifier\n",
    "preprocessor = create_preprocessor()\n",
    "classifier = create_classifier()\n",
    "# Create the pipeline\n",
    "pipeline = create_pipeline(preprocessor, classifier)\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Train the pipeline on the training data\n",
    "train_pipeline(pipeline, X_train, y_train)\n",
    "# Evaluate the model on the testing data\n",
    "evaluate_pipeline(pipeline, X_test, y_test)\n",
    "# Save the trained pipeline to a file\n",
    "save_pipeline(pipeline, 'trained_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.8878504672897196\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_model(data_source, target_column, save_model_filename):\n",
    "    # Function to train and evaluate the model given a data source\n",
    "    # Fetch dataset\n",
    "    dataset= fetch_ucirepo(id=data_source) \n",
    "    \n",
    "    X_ = dataset.data.features \n",
    "    y_ = dataset.data.targets \n",
    "    # Merge X and y into a single DataFrame\n",
    "    df = pd.concat([X_, y_], axis=1)\n",
    "    df = handle_duplicates(df)\n",
    "    \n",
    "    # Separate features and target variable\n",
    "    X, y = separate_features_target(df, target_column)\n",
    "    \n",
    "    # Identify numerical and categorical features\n",
    "    numerical_features, categorical_features = identify_features(X)\n",
    "    \n",
    "    # Create preprocessor and classifier\n",
    "    preprocessor = create_preprocessor()\n",
    "    classifier = RandomForestClassifier()\n",
    "    # Create the pipeline\n",
    "    pipeline = create_pipeline(preprocessor, classifier)\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    # Train the pipeline on the training data\n",
    "    train_pipeline(pipeline, X_train, y_train)\n",
    "    # Evaluate the model on the testing data\n",
    "    accuracy = evaluate_pipeline(pipeline, X_test, y_test)\n",
    "    # Save the trained pipeline to a file\n",
    "    save_pipeline(pipeline, save_model_filename)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def separate_features_target(df, target_column):\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "    return X, y\n",
    "\n",
    "def identify_features(X):\n",
    "    numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns\n",
    "    return numerical_features, categorical_features\n",
    "\n",
    "def handle_duplicates(df):\n",
    "    df_no_duplicates = df.drop_duplicates()\n",
    "    return df_no_duplicates\n",
    "\n",
    "def create_preprocessor():\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "    return preprocessor\n",
    "\n",
    "def create_classifier():\n",
    "    return RandomForestClassifier()\n",
    "\n",
    "def create_pipeline(preprocessor, classifier):\n",
    "    return Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "\n",
    "def train_pipeline(pipeline, X_train, y_train):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "def evaluate_pipeline(pipeline, X_test, y_test):\n",
    "    accuracy = pipeline.score(X_test, y_test)\n",
    "    print(f'Model Accuracy: {accuracy}')\n",
    "\n",
    "def save_pipeline(pipeline, file_path):\n",
    "    joblib.dump(pipeline, file_path)\n",
    "\n",
    "def load_pipeline(file_path):\n",
    "    return joblib.load(file_path)\n",
    "\n",
    "def predict_with_pipeline(pipeline, new_data):\n",
    "    return pipeline.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(id, output_filename):\n",
    "    # Fetch dataset\n",
    "    dataset = fetch_ucirepo(id=id) \n",
    "    # Extract features and targets\n",
    "    X_ = dataset.data.features \n",
    "    y_ = dataset.data.targets \n",
    "    # Merge X and y into a single DataFrame\n",
    "    df = pd.concat([X_, y_], axis=1)\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(output_filename, index=False)\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(file_path, target_column, save_model_filename, classifier):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = handle_duplicates(df)\n",
    "    # Separate features and target variable\n",
    "    X, y = separate_features_target(df, target_column)\n",
    "    # Identify numerical and categorical features\n",
    "    numerical_features, categorical_features = identify_features(X)\n",
    "    # Create preprocessor\n",
    "    preprocessor = create_preprocessor()\n",
    "    # Use the specified classifier or default to RandomForestClassifier\n",
    "    if classifier is None:\n",
    "        classifier = RandomForestClassifier()\n",
    "    # Create the pipeline\n",
    "    pipeline = create_pipeline(preprocessor, classifier)\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    # Train the pipeline on the training data\n",
    "    train_pipeline(pipeline, X_train, y_train)\n",
    "    # Evaluate the model on the testing data\n",
    "    accuracy = evaluate_pipeline(pipeline, X_test, y_test)\n",
    "    # Save the trained pipeline to a file\n",
    "    save_pipeline(pipeline, save_model_filename) \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def predict_save_csv(model_filename, new_data_filename, output_filename):\n",
    "    # Load the pre-trained pipeline\n",
    "    loaded_pipeline = load_pipeline(model_filename)\n",
    "    # Read the new data\n",
    "    new_data = pd.read_csv(new_data_filename)\n",
    "    # Predict with the loaded pipeline\n",
    "    predictions = predict_with_pipeline(loaded_pipeline, new_data)\n",
    "    predictions_df = pd.DataFrame(predictions, columns=['Predicted_Label'])\n",
    "    # Save predictions to a CSV file\n",
    "    predictions_df.to_csv(output_filename, index=False)\n",
    "    print(\"prediction is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_fetch = 890\n",
    "output_csv_filename = 'train_data.csv'\n",
    "load(id_to_fetch, output_csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.8808411214953271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{classifier} model accuracy {accuracy}'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = './train_data.csv'\n",
    "target_column = 'cid'\n",
    "save_model_filename = 'trained_pipeline.joblib'\n",
    "train_and_evaluate_model(file_path, target_column, save_model_filename, classifier=RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction is done\n",
      "Predictions on new data saved to predictions.csv\n"
     ]
    }
   ],
   "source": [
    "model_filename = 'trained_pipeline.joblib'\n",
    "new_data_filename = 'test_data.csv'\n",
    "output_filename = 'predictions.csv'\n",
    "predictions = predict_save_csv(model_filename, new_data_filename, output_filename)\n",
    "print(f'Predictions on new data saved to {output_filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
